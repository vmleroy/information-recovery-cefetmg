{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatorio - Indexador\n",
    "\n",
    "**Alunos:** *Arthur Severo e Victor Le Roy*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quais foram os principais desafios e soluções?\n",
    "\n",
    "1. O principal desafio encontrado para o problema foi durante a execução para a criação do índice das paginas da Wikipédia, mais especificamente durante o salvamento de arquivos no HD. Esse problema foi resolvido simplesmente alterando a execução para acontecer utilizando o `HashIndex`. Uma outra possível solução é a execução do programa em um ssd, uma vez que possui a escrita e a leitura consideravelmente superior à do HD.\n",
    "\n",
    "2. Um outro desafio foi com relação ao reset da lista temporária, porém apenas era falta de entendimento do projeto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qual é a vantagem/desvantagem das suas soluções sob as outras alternativas (por exemplo, uso do índice em memória principal x ocorrência de termos em memória secundária)?\n",
    "\n",
    "1. O tempo de execução em memoria principal é consideravelmente mais rápido do que a execução em memória secundária devido a leitura e a escrita superior.\n",
    "\n",
    "2. O custo de espaçamento em HD para a execução em memória secundária é muito alto.\n",
    "\n",
    "3. O uso do índice em memória secundária é uma opção boa por conta da possibilidade de retomar o progresso da indexação com base no último arquivo criado caso aconteça algum erro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O que você melhoraria no seu código para diminuir o consumo de memoria ou deixá-lo mais eficiente?\n",
    "\n",
    "1. Para o consumo de memória secundária, uma forma de diminuir o consumo é criar uma lógia que posa apagar os arquivos mais antigos e sempre manter o mais atual, uma vez que somente ele será usado, tanto para backup, quanto para a execução da indexação."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Quais são as bibliotecas externas utilizadas? Explique o funcionamento da técnica de stemming adotada.\n",
    "\n",
    "As bibliotecas externas: `nltk (SnowballStemmer e word_tokenize), bs4 (BeautifulSoup), pickle (dump e load)`\n",
    "\n",
    "Para cada página coletada, foi extraído seu conteúdo (removendo as tags HTML). Depois o texto foi transformado para LOWERCASE  removido o acento, caso perform_accents_removal = True. Em seguida foi feito o word_tokenize gerando um vetor para cada token e para cada token gerado foi verificado se ele não é acento, e se não é stopword. Por fim, o stemming, que é a remoção do radical de cada termo.\n",
    "\n",
    "Na implementação do método stem definido para linguagem portuguesa do SnowballStemmer, são definidas 4 listas de sufixos, sendo eles:\n",
    "\n",
    "1. sufixos padrões conhecidos da linguagem (como \"-amentos\", \"-amente\", \"-adora\")\n",
    "2. sufixos verbais (como \"-eremos\", \"-iriam\", \"-assem\")\n",
    "3. uma regra arbitrária que, caso o radical restante termina com \"i\" e a palavra restante (após remover nas etapas anteriores) tiver como penúltima letra o C, a última letra dos dois elementos é removida\n",
    "4. Algusn sufixos residuais são removidos caso as etapas anteriores não tenham sido executadas (ex: \"-os\", \"-a\", \"-i\", \"-o\")\n",
    "5. São tratados casos particulares de palavras terminadas com \"gu\" e \"ci\". Além disso, letras como \"ç\", \"ã\" e \"õ\" são normalizadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qual foi a estrutura do índice utilizado? Quanto MB de ram cada solução de índice gastou? Em quanto tempo foi realizado a indexação? Qual foi a média por documento?\n",
    "\n",
    "No outro arquivo jupyter, foi realizado 2 testes de desempenho para ver como o indexador se comportaria utilizando a memoria principal e a memoria secundaria.\n",
    "\n",
    "Na memoria primaria:\n",
    "\n",
    "```\n",
    "Indexação de termo 96 done\n",
    "Items per second: 95188.04596789388\n",
    "#240000 itens -  Indexação de termo done in 2.5213249999999996\n",
    "Total time elapsed: 2.5213249999999996 s\n",
    "Memoria usada: 27.122441291809082 MB; Máximo 27.125436782836914 MB\n",
    "```\n",
    "\n",
    "Na memoria secundaria:\n",
    "\n",
    "```\n",
    "Indexação de termo 96 done\n",
    "Items per second: 30481.996570775384\n",
    "#240000 itens -  Indexação de termo done in 7.873500000000001\n",
    "Total time elapsed: 7.873500000000001 s\n",
    "Memoria usada: 18.846447944641113 MB; Máximo 32.31215763092041 MB\n",
    "```\n",
    "\n",
    "Já para a indexação dos arquivos da Wikipédia, não foi realizado um teste de desempenho, porém foi utilizado a execução do HashIndex e sua execução ocorreu em cerca de 15 minutos. Para testar se o [wiki_idx](https://github.com/vmleroy/information-recovery/blob/main/Indexador/wiki_idx.rar)  foi feito corretamente, foi construido um buscador extremamente simples. Para executar, rode o código `python .\\index\\wikipedia_indexer.py no terminal` no terminal e selecione a opção 2 após o download ou indexação do arquivo.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
