{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seeds utilizadas nos testes foram as seguites:\n",
    "* http://www.globo.com/\n",
    "* http://www.cnpq.br/\n",
    "* http://www.uol.com.br/\n",
    "* https://www.uai.com.br/\n",
    "* https://www.terra.com.br/\n",
    "* https://www.bbc.com/\n",
    "* https://www.tecmundo.com.br/\n",
    "* https://www.olhardigital.com.br/\n",
    "* https://www.estadao.com.br/ \n",
    "* https://www.em.com.br/\n",
    "* https://www.gazetadopovo.com.br/\n",
    "* https://www.correiobraziliense.com.br/\n",
    "\n",
    "O output dos testes pode ser encontrado no diretorio \"results\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```Teste numero 2:```\n",
    "\n",
    "*Utilizar os seguintes parâmetros no coletor:*\n",
    "\n",
    "→ Número máximo de páginas (50.000 páginas)\n",
    "\n",
    "→ Profundidade por domínio (6 páginas)\n",
    "\n",
    "→ Número de threads utilizado\n",
    "\n",
    "*Use pelo menos 3 sementes*\n",
    "\n",
    "*Produzir um relatório em jupyter a ser definido na próxima seção*\n",
    "\n",
    "*Armazenar a lista de URLs coletadas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.crawler_tests import crawler_50k_pages\n",
    "crawler_50k_pages()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```Teste da velocidade das threads```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```Teste com 1 milhao de paginas salvas em \"pages\"```\n",
    "\n",
    "Neste teste foi realizado a varredura de 1 milhão de páginas sem parar utilizando as mesmas URLS anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.crawler_tests import crawler_1M_pages_saving_files\n",
    "crawler_1M_pages_saving_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A varredura anterior ocorreu tudo bem, porém, foram armazendas, aproximadamente, 960 mil páginas. Isto aconteceu devido a algum erro interno ocorrido, que provavelmente foi relacionado ao acesso de uma thread a alguma pagina já analisada anteriormente. Desta forma, para finalizar o procedimento de salvar as 1 milhão de páginas, será realizado mais uma varredura em 50 mil páginas com domínios diferentes do utilizado anteriormente, para garantir, ou ao menos tentar previnir, que seja buscado uma página já salva anteriormente. As novas URLs são:\n",
    "\n",
    "* https://www.youtube.com/\n",
    "* https://crunchyroll.com/\n",
    "* https://www.reddit.com\n",
    "\n",
    "O print a seguir demonstra o resultado obtido na primeira varredura. Uma observação a ser destacada deste print é que o Jupyter deu bug enquanto rodava o crawler e, por conta disso, o output foi deletado, porém ainda foi possível resgatar o output como txt, que está salvo no arquivo \"1M-pages-output-1k-threads.rar\". Além disso, o tempo percorrido para finalizar o processo está presente no code do Jupyter.\n",
    "\n",
    "<img style=\"float:left\" src=\"imgs/1M-pages-test-final-result.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.crawler_tests import crawler_50k_pages_saving_files\n",
    "crawler_50k_pages_saving_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a execução do ultimo teste, foi possível obter 1 milhão de arquivos salvos, como mostrado no print abaixo:\n",
    "\n",
    "<img style=\"float:left\" src=\"imgs/1M-pages-completed.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada arquivo foi salvo da seguinte maneira:\n",
    "\n",
    "* 1a linhas --> URL: https://www.google.com\n",
    "* Resto do arquivo --> HTML da página requisitada\n",
    "\n",
    "<img style=\"float:left\" src=\"imgs/example-saved-page.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
